import os
import json
import re
import requests

# ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ø³Ø¹Ø© Ù„Ø¶Ù…Ø§Ù† Ø¬Ù„Ø¨ Ø£ÙƒØ¨Ø± Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ù‚Ù†ÙˆØ§Øª
TARGET_KEYWORDS = ["beIN", "SSC", "KSA", "Alkass", "AD SPORT", "Sport", "Live", "Yalla", "Shoot"]
OUTPUT_FILE = "channels.json"

def check_link(url):
    """ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ø´ØºØ§Ù„Ø§Ù‹ Ø£Ù… Ù„Ø§"""
    try:
        # Ù†Ø±Ø³Ù„ Ø·Ù„Ø¨ ÙØ­Øµ Ø¨Ù…Ø¯Ø© Ø§Ù†ØªØ¸Ø§Ø± Ù‚ØµÙŠØ±Ø© (3 Ø«ÙˆØ§Ù†ÙŠ) Ù„ÙƒÙŠ Ù„Ø§ ÙŠØªØ£Ø®Ø± Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª
        response = requests.get(url, timeout=3, stream=True)
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ø¯Ø£ Ø¨Ù€ 200 ÙŠØ¹Ù†ÙŠ Ø§Ù„Ø±Ø§Ø¨Ø· Ø´ØºØ§Ù„
        return response.status_code == 200
    except:
        return False

def fetch_and_clean():
    all_raw_channels = []
    print("ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©...")

    # 1. ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³Ø­ÙˆØ¨Ø© (ÙŠÙ„Ø§ Ø´ÙˆØª)
    for root, dirs, files in os.walk("."):
        for file in files:
            if file.endswith((".js", ".html", ".txt")):
                try:
                    with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                        content = f.read()
                        links = re.findall(r'https?[:\/\\]+[^\s"\']+\.m3u8[^\s"\']*', content)
                        for l in links:
                            clean_url = l.replace('\\/', '/').replace('\\', '')
                            all_raw_channels.append({"name": f"Live {file[:5]}", "url": clean_url})
                except: continue

    # 2. ÙØ­Øµ Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª GitHub Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©
    github_sources = [
        "https://iptv-org.github.io/iptv/countries/ar.m3u",
        "https://raw.githubusercontent.com/skid9000/All-In-One-IPTV/main/All-In-One-IPTV.m3u",
        "https://raw.githubusercontent.com/Moebis-Iptv/M3U/main/Arabic.m3u"
    ]
    for src in github_sources:
        try:
            r = requests.get(src, timeout=5)
            matches = re.findall(r'#EXTINF.*?,(.*?)\n(http.*)', r.text)
            for name, url in matches:
                all_raw_channels.append({"name": name.strip(), "url": url.strip()})
        except: pass

    # 3. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„ÙØ­Øµ Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
    unique_links = {c['url']: c for c in all_raw_channels}.values()
    print(f"ğŸ” ÙˆØ¬Ø¯Ù†Ø§ {len(unique_links)} Ø±Ø§Ø¨Ø· ÙØ±ÙŠØ¯. Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„Ø´ØºØ§Ù„ Ù…Ù†Ù‡Ø§ Ø§Ù„Ø¢Ù†...")

    # 4. Ø§Ù„ÙØ­Øµ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ (Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£Ù‡Ù…)
    final_working_channels = []
    for chan in unique_links:
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚Ù†Ø§Ø© Ø±ÙŠØ§Ø¶ÙŠØ© Ø£Ùˆ Ù…Ù† Ù…Ù„ÙØ§ØªÙƒØŒ Ø³Ù†ÙØ­ØµÙ‡Ø§
        if any(word.upper() in chan['name'].upper() for word in TARGET_KEYWORDS) or "Live" in chan['name']:
            if check_link(chan['url']):
                print(f"âœ… Ø´ØºØ§Ù„Ø©: {chan['name']}")
                final_working_channels.append({
                    "name": chan['name'],
                    "url": chan['url'],
                    "logo": "https://cdn-icons-png.flaticon.com/512/716/716429.png"
                })
            else:
                print(f"âŒ Ù…Ø¹Ø·Ù„Ø©: {chan['name']}")

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_working_channels, f, ensure_ascii=False, indent=4)
    
    print(f"âœ¨ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ØªÙ…Øª! ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(final_working_channels)} Ù‚Ù†Ø§Ø© Ø´ØºØ§Ù„Ø©.")

if __name__ == "__main__":
    fetch_and_clean()
